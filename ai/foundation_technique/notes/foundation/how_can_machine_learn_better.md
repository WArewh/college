# 如何让机器更好学习

## 过拟合
过拟合：$E_{in}(h)$很小，但$E_{out}(h)$很大  

### 原因
1. 数据量少
2. 随机噪音多
3. 高斯噪音多
4. 实际模型次数低

### 解决
1. 简单模型
2. 数据清洗（改正标签）/剪裁（去掉数据）
3. 数据提示（对原数据进行改变，添加人造数据）
4. 正则化
5. 验证

## 正则化
正则化把降次的问题转换成带有限制条件的问题，可以使用拉格朗日乘数法求解

### 减少模型复杂度
简单理解:  
模型过于复杂是因为模型尝试去兼顾各个测试数据点，处于一种动荡的状态，每个点的到时在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数绝对值非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。加入惩罚项后，只要控制λ的大小，当λ很大时，θ就会很小，即达到了约束数量庞大的特征的目的  

<!-- 贝叶斯：
正则化是为模型参数估计增加一个先验知识，先验知识会引导损失函数最小值过程朝着约束方向迭代，因此整个最优化问题可以看做是一个最大后验估计，其中正则化项对应后验估计中的先验信息，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计 -->


### L1正则化
<!-- 假设w服从零均值拉普拉斯分布   -->
L1更容易得到稀疏解可以看[这篇文章](https://www.zhihu.com/question/37096933/answer/70426653)，因此L1就是通过舍弃掉一些不重要的特征（解向量0多）来达到目的

![L1](https://pic2.zhimg.com/80/v2-3fef81c912c4ac0fd8e61a007139f855_720w.jpg)


### L2正则化
<!-- 假设w服从零均值正态分布 -->
L2比L1更加平滑，因此L2比较容易得到0附近的解，因此L2就是控制所有特征的权重来达到目的

## 验证
将训练数据预留出一部分作为选择参数的验证数据，可用于模型调参、评估、特征选择

### 模型选择
1. 将数据集D分为训练集$D_{train}$和验证集$D_{test}$。
2. 使用不同的模型进行训练得到这个模型下的最佳假设。
3. 再使用验证数据集，选择验证错误最小的，得出最佳模型
4. 最后通过该模型训练整个数据集D，得到最终的假设

![模型选择](https://images2015.cnblogs.com/blog/489652/201602/489652-20160211152217698-291165867.png)

### 简单交叉验证
我们随机的将数据分为两部分（比如： 80%训练20%验证），最终结果和分组有一定关系

### K折交叉验证
K一般取10，如果取N则为留一交叉验证  
把数据切分成K份，不重复的选一个样本作为验证集，其他K-1个样本用来训练，进行K次，取平均得到模型估测，这样保证每个样本参与训练且被测试，降低了泛化误差

![10折交叉验证](https://pic1.zhimg.com/80/v2-c894c9cdf08852339acf372820ef5d6c_720w.jpg)

## 三大原则
1. 使用的模型先易后难
2. 训练样本和测试样本要都独立同分布的来自于概率分布P，不要有偏差
3. 不要数据窥探