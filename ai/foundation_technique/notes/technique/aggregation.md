# 聚合模型
基本思想就是使用不同的模型g来合成一个模型G，类似概率SVM的两阶段学习  
聚合多个比较弱的模型可以产生比较强大的模型，相当于增加模型复杂度
聚合多个不同的模型可以产生比较中庸的结果，相当于正则化  
因此可以聚合模型可以得到更好的性能
![聚合](https://img2018.cnblogs.com/blog/1010007/201906/1010007-20190619154053780-1986069632.png)

# 融合blending
存在多个已知的模型g进行聚合

## 平均融合
对模型g输出结果求平均或最大频率

## 线性融合
对模型g输出的结果线性组合

### 损失函数
线性融合的损失函数基本同线性函数的损失函数，比如线性回归和线性融合回归的损失函数就是平方误差，只不过线性融合回归的x为g(x)，相当于将假设进行转换

### 训练
首先使用训练集训练出$g^{-}$，再使用验证集训练出系数，最后整体训练出g，使用验证集得到的参数和g，以此得到G  
![原理](https://img2018.cnblogs.com/blog/1010007/201906/1010007-20190619160820072-606793019.png)

## 堆叠融合
对模型g输出的结果用任意的方式进行组合，不局限于线性组合，这种方式也称为stacking

## g如何多样
1. 使用不同的模型
2. 不同的超参数
3. 算法的随机性，比如pla
4. 数据的随机性，比如交叉验证得到的$g^{-}_v$
5. 更多数据
6. 不同的特征

# 学习learning
聚合的同时也在得到假设g

## bagging
利用bootstrap进行aggragation的操作就被称为bagging，如果base algorithm对数据的随机性敏感，那么bagging的效果将会很好

### bootstrap
从已有数据集D中模拟出不一样的数据集$D_t$，其实就是随机的有放回抽样，即一个样本可能被多次抽到，没有被抽到的资料就叫做OOB（Out Of Bag）  
bootstrap被称为一个meta algorithm，用于训练的算法A称为base algorithm

## adaboost
核心思想是针对同一个训练集训练不同的弱分类器，然后把这些弱分类器集合起来，构成一个更强的分类器

### 得到不同分类器
![不同分类器](https://images2015.cnblogs.com/blog/1020614/201703/1020614-20170304203859313-690179126.png)
可以控制每个样本的权重使得$g_t$在$g_{t+1}$的数据集上效果很差（正确率0.5），要想达到这种效果可以设置$\blacklozenge_t=\sqrt{\frac{1-\varepsilon_t}{\varepsilon_t}}$，错误点乘$\blacklozenge_t$，正确点除$\blacklozenge_t$

### 聚合
adaboost使用线性融合
![adaboost](https://images2015.cnblogs.com/blog/1020614/201703/1020614-20170304210854235-622097062.png)

## 决策树
采用分而治之的思想，将大数据集分成小数据集，再从小数据集学习到的模型合成一个大模型
![基本决策树](https://img-blog.csdn.net/20180331221329144)
每一棵决策树需要回答四个问题：
1. 分支个数C
2. 分支条件b(x)
3. 终止条件
4. 基假设函数$g_t(x)$

### CART
1. 二叉树
2. 使用decision stump分开，计算两个分支的不纯度并求平均，以此确定分支
3. 终止条件为不纯度为0（$y_n$都一样）或者只有重复点（$x_n$都一样）
4. 基假设函数为常数

#### 不纯度
不纯度类似损失函数，回归一般使用平方错误，分类一般使用基尼系数（Gini index）或者分类错误（classification error）  
分类错误:
$$
1-\max_{1\le k\le K}\frac{\sum^{N}_{n=1}[y_n=k]}{N}
$$
基尼系数:
$$
1-\sum^{K}_{k=1}\biggl(\frac{\sum^{N}_{n=1}[y_n=k]}{N}\biggr)^2
$$



#### 正则化
可以使用叶子个数进行正则化
$$
\Omega(G)为G的叶子个数\\
arg\min_{G} E_{in}(G) + \lambda\Omega(G)
$$
将所有G都算出来是不可能的，CA&T采用以下策略：
$$
\begin{array} { l } G ^ { ( 0 ) } = \text { fully-grown tree } \\ G ^ { ( i ) } = \operatorname { argmin } _ { G } E _ { \text {in } } ( G ) \text { such that } G \text { is one-leaf removed from } G ^ { ( i - 1 ) } \end{array}
$$
大概意思就是在 摘掉i片树叶的决策树中找出性能最优的一颗，该决策树由摘掉i−1片树叶的决策树中最优的一颗再摘掉一片获得，从$G^{(i)}$里找到性能最优的一棵

#### 特点
- 很容易处理离散特征，比如分类一定存在一棵树把所有离散点都分类正确
- 很容易处理特征缺失问题，可以用其他条件替代

#### 比较
AdaBoost-Stump是在全局数据上进行分割，而决策树则是在条件上在进行分割

## 随机森林
随机森林是bagging和完全决策树的结合，可以得到一个既稳定又强大的模型

### 如何多样化
在构建分支时可以对特征随机地进行选择和组合

### OOB检测
查看哪些样本是哪些$g_t$的OOB，计算在这些$g_t$上的损失求平均，类似于留一验证
$$
E_{oob}(G)=\frac{1}{N}\sum^{N}_{i=1}err(y_n,G^-_n(x_n))
$$
$E_{oob}$也称为随机森林的自验证（不用划分数据集），一般来说，这个错误是很准的

### 特征选择
通过计算特征重要性来进行特征选取，如果将一个特征改变，最后的效果很差，那么这个特征就很重要，在随机森林中主要使用random test来进行选取

#### random test
主要思想是将原来的值替换一个随机值，这个随机方法可以是常见的概率分布，但可能和原来的分布不一致，因此采用permutation test，将这一特征洗牌，这样就保证了分布一致。  
重要度计算:
$$
important(i) = performance(D) - performance(D^{(p)})\\
D^{(p)}\ is\ D\ with\ \{x_{n,i}\}\ replaced\ by\ permuated\ \{x_{n,i}\}^N_{i=1} 
$$

#### 优化
如果改变特征就要重新训练，这就导致计算量很大，为了简化可以使用OOB来计算。也就是说，训练的时候使用D，但在OOB验证时，将特征洗牌，验证G的效果

### 效果
理论上，树的个数越多，效果会越好，会产生类似于SVM的效果，由于是一个随机算法因此效果和开始的初始值有关

# GBDT
跳过