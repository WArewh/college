# 学习的可行性
学习是可能做不到的，在训练集中，可以求得一个最佳的假设h，该假设最大可能的接近目标函数f，但是训练集之外的数据h和f可能差别很远。

## 可能的补救
通过少量的已知样本推论整个样本集，即通过概率统计解决，假设总体预测错误概率为$u$，总体预测正确的概率为$1-u$，训练集预测错误概率为$v$，训练集预测正确概率为$1-v$，根据霍夫丁不等式可得，当N足够大时，$u$近似等于$v$
$$
v=E_{in}(h)=\frac{1}{N}\sum^{N}_{i=1}(h(x)\ne f(x)) \\

u=E_{out}(h)=E_{x \  \sim \ p \ }(h(x)\ne f(x)) \\

P\left[\vert u-v \vert \lt \epsilon \right] = P\left[\vert E_{in}(h)-E_{out}(h) \vert \lt \epsilon \right] \le 2exp(-2\epsilon^2N)
$$

![引入统计](https://images0.cnblogs.com/blog/489652/201502/060921545621382.png)

坏样本：抽样中存在$E_{in}(h)$和$E_{out}(h)$相差很大的样本。  

霍夫丁不等式保证坏数据出现很少，但是根据不同的假设，坏数据并不一样，假设有M个假设，那么出现坏样本的概率上限为$2M·exp(-2\epsilon^2N)$  
如果假设有限、数据够多，那么不管用什么算法，都可以可以满足$E_{in}(h)=E_{out}(h)$，那么最好假设就是$E_{in}(h)$最小的那个假设  

### 可学习的两个条件
1. 假设中可以挑选出来一个h，使得$E_{in}(h)=0$
2. 假设有限，当数据足够大时，$E_{in}(h)=E_{out}(h)$

### 减少假设上限
见[这篇文章](https://www.cnblogs.com/ymingjingr/p/4285358.html)以及[这篇文章](https://www.cnblogs.com/ymingjingr/p/4290983.html)

## VC维
$d_{VC}$为最大非突破点，即突破点-1，约等于自由度(可以理解为参数个数)  
$d_{VC}$有限即有突破点，则满足可学习条件2  

### 解释
$d_{VC}$越大，则说明自由度越大，那么就需要更多的数据  
训练数据量应该约等于$10d_{VC}$